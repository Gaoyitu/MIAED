import tensorflow as tf
import data
import numpy as np
import models
from sklearn.preprocessing import MinMaxScaler



def get_testing_data(x,x_noise,x_adv,y,classifier):
    y1 = np.reshape(y, (y.shape[0],))
    preds = classifier.predict(x)
    preds_noise = classifier.predict(x_noise)

    inds_correct = np.where(preds.argmax(axis=1) == y1)[0]
    inds_correct_noise = np.where(preds_noise.argmax(axis=1) == y1)[0]

    inds_correct = inds_correct[np.in1d(inds_correct, inds_correct_noise)]

    x_adv = x_adv[inds_correct]
    y = y[inds_correct]
    y1= np.reshape(y,(y.shape[0],))

    x = x[inds_correct]
    x_noise = x_noise[inds_correct]

    preds_adv = classifier.predict(x_adv)
    inds_correct = np.where(preds_adv.argmax(axis=1) == y1)[0]

    x_adv = np.delete(x_adv,inds_correct,axis=0)
    x = np.delete(x,inds_correct,axis=0)
    x_noise = np.delete(x_noise,inds_correct,axis=0)
    y = np.delete(y,inds_correct,axis=0)

    return x,x_noise,x_adv,y



def get_classifier(dropout,vector_dimension):

    z = tf.keras.layers.Input(shape=(vector_dimension,))
    y = tf.keras.layers.Dense(1024)(z)
    y = tf.keras.layers.BatchNormalization()(y)
    y = tf.keras.layers.ReLU()(y)
    y = tf.keras.layers.Dropout(dropout)(y)
    y = tf.keras.layers.Dense(512)(y)
    y = tf.keras.layers.BatchNormalization()(y)
    y = tf.keras.layers.ReLU()(y)
    y = tf.keras.layers.Dropout(dropout)(y)
    y = tf.keras.layers.Dense(10, activation='softmax')(y)
    model = tf.keras.Model(inputs=z, outputs=y)

    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
    return model

def get_encoder(encoder_weights,fc_model_weights,vector_dimension):
    encoder = models.get_mnist_encoder()
    fc_model = models.get_mnist_full_connection_layers(vector_dimension)
    encoder.load_weights('networks/mnist/autoencoder/'+encoder_weights+'.h5')
    fc_model.load_weights('networks/mnist/autoencoder/'+fc_model_weights+'.h5')

    model = tf.keras.models.Sequential()
    model.add(encoder)
    model.add(fc_model)

    return model
def train_classifier(encoder_name,fc_model_name,savefile):
    vector_dimension = 64
    x_train, y_train, x_test, y_test = data.get_mnist()
    x_adv = np.load('G:/adversarial_examples/mnist/local/x_test_adv_FGSM.npy')

    classifier_d = get_classifier(0.5, vector_dimension)

    encoder = get_encoder(encoder_name, fc_model_name, vector_dimension)

    vectors = encoder.predict(x_train)
    x_adv_vectors = encoder.predict(x_adv)
    x_vectors = encoder.predict(x_test)

    train_callbacks = [
        tf.keras.callbacks.EarlyStopping(
        monitor="val_loss", patience=20,
        restore_best_weights=True
        ),
        tf.keras.callbacks.ReduceLROnPlateau(
            monitor="val_loss", factor=0.5,
            patience=3, verbose=1
        ),

    ]

    classifier_d.fit(vectors,y_train,batch_size=256,epochs=100,validation_data=(x_adv_vectors,y_test),callbacks=train_callbacks)

    classifier_d.evaluate(x_adv_vectors,y_test,verbose=2)

    classifier_d.save_weights('networks/mnist/autoencoder/'+savefile+'.h5')

def test_mnist(encoder_name, fc_model_name, simple_classifier_name,classifier):
    vector_dimension = 64

    # classifier = models.get_mnist_local()

    _, _, x_test, y_test = data.get_mnist()

    classifier_d = get_classifier(0, vector_dimension)

    classifier_d.load_weights('networks/mnist/autoencoder/'+simple_classifier_name+'.h5')

    encoder = get_encoder(encoder_name, fc_model_name, vector_dimension)

    model_names = ['black']
    adv_methods = ['FGSM', 'MIM', 'PGD', 'BIM', 'CW2']

    for i in range(len(model_names)):
        for j in range(len(adv_methods)):
            print('adversarial examples generated by', model_names[i], '+', adv_methods[j])
            x_noise = np.add(x_test, np.random.normal(0, 0.1, x_test.shape))
            x_adv = np.load(
                'G:/adversarial_examples/mnist/' + model_names[i] + '/x_test_adv_' + adv_methods[j] + '.npy')

            x, x_n, x_a, y = get_testing_data(x=x_test, x_noise=x_noise, x_adv=x_adv, y=y_test,
                                              classifier=classifier)

            x_vectors = encoder.predict(x)
            x_a_vectors = encoder.predict(x_a)
            x_n_vectors = encoder.predict(x_n)

            x_equal_num = tf.reduce_sum(
                tf.cast(tf.equal(tf.argmax(classifier.predict(x), 1), tf.argmax(classifier_d.predict(x_vectors), 1)),
                        dtype=tf.float32))
            x_a_equal_num = tf.reduce_sum(tf.cast(
                tf.equal(tf.argmax(classifier.predict(x_a), 1), tf.argmax(classifier_d.predict(x_a_vectors), 1)),
                dtype=tf.float32))
            x_n_equal_num = tf.reduce_sum(
                tf.cast(
                    tf.equal(tf.argmax(classifier.predict(x_n), 1), tf.argmax(classifier_d.predict(x_n_vectors), 1)),
                    dtype=tf.float32))

            positive_number = x.shape[0]*2
            negative_number = x_a.shape[0] * 2

            TP = positive_number - x_a_equal_num - x_a_equal_num
            FN = x_a_equal_num*2
            FP = negative_number - x_equal_num - x_n_equal_num
            TN = x_equal_num + x_n_equal_num

            recall = TP / positive_number
            precision = TP / (TP + FP)
            specificity = TN / negative_number
            accuracy = (TP + TN) / (positive_number + negative_number)
            F1 = 2 * (recall * precision) / (recall + precision)

            print('sample number:{}'.format(x.shape[0]), end=' ')
            print('test_score:', x_equal_num.numpy(), end=' ')
            print('noise_score:', x_n_equal_num.numpy(), end=' ')
            print('adv_score:', (x.shape[0] - x_a_equal_num).numpy(), end=' ')
            print('recall:{0:0.4f}'.format(recall.numpy()), end=' ')
            print('precision:{0:0.4f}'.format(precision.numpy()), end=' ')
            print('F1:{0:0.4f}'.format(F1.numpy()), end=' ')
            print('specificity:{0:0.4f}'.format(specificity.numpy()), end=' ')
            print('accuracy:{0:0.4f}'.format(accuracy.numpy()))

def training_simple_classifier_ablations():
    ablations = ['local', 'global', 'prior', 'local_global', 'local_prior', 'global_prior']

    for ablation in ablations:
        encoder = 'mi_' + ablation + '_encoder'
        full_connection = 'mi_' + ablation + '_fc_model'
        savefile = 'mi_' + ablation + '_classifier'

        train_classifier(encoder_name=encoder, fc_model_name=full_connection, savefile=savefile)
if __name__ == '__main__':

    classifier = models.get_mnist_black()
    ablations = ['local', 'global', 'prior', 'local_global', 'local_prior', 'global_prior']


    for ablation in ablations:
        print('')
        print('')
        print('')
        print('ablation without '+ablation)
        encoder = 'mi_' + ablation + '_encoder'
        full_connection = 'mi_' + ablation + '_fc_model'
        simple_classifier = 'mi_' + ablation + '_classifier'

        test_mnist(encoder_name=encoder, fc_model_name=full_connection, simple_classifier_name=simple_classifier,classifier=classifier)












